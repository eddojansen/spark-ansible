---
- hosts: master:slaves
  vars_files:
    - vars/spark.yaml
  become: yes
  tasks:

    - name: Install packages Centos
      yum:
        name: "{{ packages }}"
        state: present
        update_cache: yes
      vars:
        packages:
        - chrony
        - atop
        - htop
        - netdata
        - irqbalance
        - java-8-openjdk
        - scala
        - git
        - unzip  
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
      tags: packages

    - name: Install packages Ubuntu
      apt:
        name: "{{ packages }}"
        state: present
        update_cache: yes
      vars:
        packages:
        - chrony
        - atop
        - htop
        - netdata
        - tuned
        - irqbalance
        - openjdk-8-jre-headless
        - scala
        - git
        - unzip  
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
      tags: packages

    - name: Ensure netdata is configured to listen on all IP's
      lineinfile:
        path: /etc/netdata/netdata.conf
        regexp: 'IP ='
        line:         bind to IP = {{ ansible_default_ipv4.address }}
      tags: packages

    - name: Restart netdata
      service:
        name: netdata
        state: restarted
        enabled: yes
      tags: packages

    - name: Ensure Chrony is running
      service:
        name: chronyd
        state: restarted
        enabled: yes
      ignore_errors: yes
      tags: packages

    - name: Stop and Disable Firewalld
      service:
        name: firewalld
        state: stopped
        enabled: no
      ignore_errors: yes

    - name: Disable SE Linux
      shell:
        cmd: /usr/sbin/setenforce 0
      register: command_result
      failed_when: "'ERROR' in command_result.stderr"
      ignore_errors: yes
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
      tags: prep

    - name: Disable SELinux permanently
      selinux:
        state: disabled
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
      tags: prep

    - name: Ensure Apparmor is stopped
      service:
        name: apparmor
        state: stopped
        enabled: no
      ignore_errors: yes
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
      tags: prep

    - name: Remove apparmor
      package:
        name: apparmor
        state: absent
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
      ignore_errors: yes
      tags: prep

    - name: Remove swaps from fstab
      lineinfile:
        dest: /etc/fstab
        regexp: '^/[\S]+\s+swap\s+swap'
        state: absent
      tags: tune

    - name: Disable swap
      shell:
        cmd: swapoff -a
      tags: tune

    - name: Ensure irqbalance is set to oneshot - Centos
      lineinfile:
        path: /etc/sysconfig/irqbalance
        regexp: '^#IRQBALANCE_ONESHOT='
        line: IRQBALANCE_ONESHOT=1
      when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'
      tags: tune

    - name: Ensure irqbalance is set to oneshot - Ubuntu
      lineinfile:
        path: /etc/default/irqbalance
        regexp: '^#IRQBALANCE_ONESHOT='
        line: IRQBALANCE_ONESHOT=1
      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'
      tags: tune

    - name: Restart irqbalance
      service:
        name: irqbalance
        state: restarted
        enabled: yes
      tags: tune

    - name: Set tuned profile
      shell:
        cmd: tuned-adm profile throughput-performance
      tags: tune

- hosts: master
  gather_facts: true
  vars_files:
    - vars/spark.yaml
  become: no
  tasks:
 
    - name: Create spark directory
      file:
        path: '{{ CONFIG }}/spark'
        state: directory
        recurse: yes
      tags: spark

    - name: Create spark history directory
      file:
        path: '{{ CONFIG }}/spark/history'
        state: directory
        recurse: yes
      tags: spark

    - name: Create sparkRapidsPlugin directory
      file:
        path: '{{ CONFIG }}/sparkRapidsPlugin'
        state: directory
        recurse: yes
      tags: spark
      
    - name: Download and unarchive Spark
      unarchive:
        src: '{{ SPARK_DOWNLOAD_URL }}'
        dest: '{{ CONFIG }}/spark'
        remote_src: yes
        extra_opts: [--strip-components=1]
      tags: spark

    - name: Download Rapids
      get_url:
        url: '{{ RAPIDS_PLUGIN_URL }}'
        dest: '{{ CONFIG }}/sparkRapidsPlugin'
      tags: spark

    - name: Download Cudf
      get_url:
        url: '{{ CUDF_FILES_URL }}'
        dest: '{{ CONFIG }}/sparkRapidsPlugin'
      tags: spark

    - name: Download additional jars
      get_url:
        url: '{{ item }}'
        dest: '{{ CONFIG }}/spark/jars'
      with_items: 
         - '{{ JODA_URL }}'
         - '{{ HADOOP_AWS_URL }}'
         - '{{ AWS_JAVA_SDK_URL }}'
         - '{{ AWS_JAVA_SDK_CORE_URL }}'
         - '{{ AWS_JAVA_SDK_DYMODB_URL }}'
         - '{{ AWS_JAVA_SDK_S3_URL }}'
         - '{{ GCS_CONNECTOR_HADOOP_URL }}'
      tags: test

- hosts: master:slaves
  gather_facts: true
  vars_files:
    - vars/spark.yaml
  become: no
  tasks:

    - name: Set SPARK_RAPIDS_PLUGIN_JAR
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
        SPARK_RAPIDS_DIR: '{{ CONFIG }}/sparkRapidsPlugin'
      lineinfile:
        dest: /etc/environment
        line: SPARK_RAPIDS_PLUGIN_JAR="{{ item }}"
        insertafter: 'EOF'
        regexp: '{{ item }}'
        state: present
      with_fileglob:
        - '{{ CONFIG }}/sparkRapidsPlugin/rapids*'        
      become: yes
      tags: spark
 
    - name: Set SPARK_CUDF_JAR
      environment:
        SPARK_HOME: "{{ CONFIG }}/spark"
        SPARK_RAPIDS_DIR: '{{ CONFIG }}/sparkRapidsPlugin'
      lineinfile:
        dest: /etc/environment
        line: SPARK_CUDF_JAR="{{ item }}"
        insertafter: 'EOF'
        regexp: '{{ item }}'
        state: present
      with_fileglob:
        - "{{ CONFIG }}/sparkRapidsPlugin/cudf*"
      become: yes
      tags: test

    - name: Adding SPARK_HOME to /etc/environment
      lineinfile: 
        dest: /etc/environment
        line: 'SPARK_HOME={{ CONFIG }}/spark' 
        insertafter: 'EOF' 
        regexp: 'SPARK_HOME={{ CONFIG }}/spark' 
        state: present
      become: yes
      tags: spark

    - name: Adding SPARK_RAPIDS_DIR to /etc/environment
      lineinfile:
        dest: /etc/environment
        line: "SPARK_RAPIDS_DIR={{ CONFIG }}/sparkRapidsPlugin"
        insertafter: 'EOF'
        regexp: "SPARK_RAPIDS_DIR={{ CONFIG }}/sparkRapidsPlugin"
        state: present
      become: yes
      tags: spark

- hosts: master
  gather_facts: true
  vars_files:
    - vars/spark.yaml
  become: no
  tasks:

    - set_fact:
        master_ip: "{{ ansible_default_ipv4.address }}"
      tags: test
      
    - set_fact:
        slave_ip: "{{ groups['slaves'] | map('extract', hostvars, ['ansible_default_ipv4', 'address'])  | list  }}"
      tags: spark

    - name: Copy getGpusResources.sh
      environment:
        SPARK_RAPIDS_DIR: '{{ CONFIG }}/sparkRapidsPlugin'
      copy:
        src:  templates/getGpusResources.sh
        dest: $SPARK_RAPIDS_DIR/getGpusResources.sh
        backup: yes
        mode: 0777
      tags: spark

    - name: Copy spark-env.sh
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
      copy:
        src: templates/spark-env.sh
        dest: $SPARK_HOME/conf/spark-env.sh
        backup: yes
      tags: spark
   
    - name: Adding the spark master to the spark-env.sh
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
      lineinfile:
        dest: $SPARK_HOME/conf/spark-env.sh
        line: 'SPARK_MASTER_HOST="{{ master_ip }}"'
        insertafter: 'EOF'
        regexp: 'SPARK_MASTER_HOST="{{ master_ip }}"'
        state: present
      become_flags: '-l'        
      tags: spark
   
    - name: Adding the spark worker options to the spark-env.sh
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
      lineinfile:
        dest: $SPARK_HOME/conf/spark-env.sh
        line: 'SPARK_WORKER_OPTS="{{ WORKER_OPTS }}"'
        insertafter: 'EOF'
        regexp: 'SPARK_WORKER_OPTS="{{ WORKER_OPTS }}"'
        state: present
      tags: spark
   
    - name: Copy slaves
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
      copy:
        src: templates/slaves
        dest: $SPARK_HOME/conf/slaves
        backup: yes
      tags: spark

    - name: Set slave list in config file
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
      lineinfile:
        path: "$SPARK_HOME/conf/slaves"
        line: "{{ item }}"
      with_items:
        - "{{ slave_ip }}"
      tags: spark
 
    - name: Copy spark-defaults.conf
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
      copy:
        src: templates/spark-defaults.conf
        dest: $SPARK_HOME/conf/spark-defaults.conf
        backup: yes
      tags: spark

    - name: Start Spark Master & Slaves
      environment:
        SPARK_HOME: '{{ CONFIG }}/spark'
      shell: $SPARK_HOME/sbin/start-all.sh
      tags: spark
